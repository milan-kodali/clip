{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "9c26614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import tiktoken\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Shared Transformer Blocks\n",
    "# ------------------------------\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"linear layer + non-linearity to add compute after multi-head attention\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # expand onto higher dimensional space\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd) # project back down to model's embedding dimensionality \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"multiple self-attention heads in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, batched together\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, sequence length, n_embd\n",
    "        # calculate query, key, value for all heads in a batch\n",
    "        # C = n_head * head_size, eg n_head = 12, head_size = 64, so C = 768\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        q = q.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        v = v.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        \n",
    "        # use flash attention instead of manually implemented attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=self.is_decoder) # (B, n_head, T, head_size)\n",
    "        \n",
    "        y = y.transpose(1, 2).reshape(B, T, -1) # (B, n_head, T, head_size) -> (B, T, n_head * head_size)\n",
    "\n",
    "        y = self.c_proj(y) \n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"transformer block: communication followed by computation, with resid connection (x +)\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Text Decoder (GPT-style)\n",
    "# ------------------------------\n",
    "\n",
    "@dataclass\n",
    "class TextConfig:\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 512\n",
    "    out_dim: int = 512\n",
    "    is_decoder: bool = True\n",
    "    block_size: int = 77\n",
    "    vocab_size: int = 50258 # from TextTokenizer\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    \"\"\"GPT-style transformer decoder for text embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.proj = nn.Linear(config.n_embd, config.out_dim, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        # Use pytorch default LayerNorm init\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        token_emb = self.transformer.wte(idx) # (B, T, C = n_embd)\n",
    "        pos_emb = self.transformer.wpe(torch.arange(T, device=idx.device)) # (T, C = n_embd)\n",
    "        x = token_emb + pos_emb # (B, T, C = n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x) # (B, T, C = n_embd)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Find first occurrence of eot_token_id (50256)\n",
    "        # TODO: don't hardcode EOT token id\n",
    "        eot_positions = (idx == 50256).int().argmax(dim=-1)  # (B,)\n",
    "        eot_token = x[torch.arange(x.shape[0]), eot_positions]  # (B, C = n_embd)\n",
    "        out = self.proj(eot_token) # (B, C = out_dim)\n",
    "        out = out / out.norm(dim=-1, keepdim=True) # normalize to unit length for cosine similarity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a595ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Text Tokenizer\n",
    "# ------------------------------\n",
    "\n",
    "class TextTokenizer: \n",
    "    \"\"\" \n",
    "    tiktoken Wrapper\n",
    "    Not quite CLIP tokenizer, but approximates it using GPT-2 tokenizer \n",
    "    Vocab size = GPT-2 vocab size (50257) + 1 (for new SOT token) = 50258\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        # Special tokens\n",
    "        self.eot_token = \"<|endoftext|>\"\n",
    "        self.pad_token = self.eot_token\n",
    "        self.sot_token = \"<|startoftext|>\"\n",
    "        self.eot_token_id = 50256 # already exists in GPT-2 tokenizer\n",
    "        self.pad_token_id = self.eot_token_id\n",
    "        self.sot_token_id = self.eot_token_id + 1 # doesn't exist in GPT-2 tokenizer\n",
    "        \n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [self.sot_token_id]\n",
    "        text_enc = self.enc.encode(text)\n",
    "        if len(text_enc) + 2 > self.block_size:\n",
    "            tokens.extend(text_enc[:self.block_size - 2])\n",
    "        else:\n",
    "            tokens.extend(text_enc)\n",
    "            if len(tokens) < self.block_size:\n",
    "                tokens.extend([self.pad_token_id] * (self.block_size - 1 - len(tokens)))\n",
    "        tokens.extend([self.eot_token_id])\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    def decode(self, ids, include_special_tokens=True):\n",
    "        result = \"\"\n",
    "        for id in ids.tolist():\n",
    "            if id == self.sot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.sot_token\n",
    "            elif id == self.eot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.eot_token\n",
    "            else:\n",
    "                result += self.enc.decode([id])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Vision Encoder\n",
    "# ------------------------------\n",
    "\n",
    "@dataclass\n",
    "class VisionConfig:\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 768\n",
    "    out_dim: int = 512\n",
    "    is_decoder: bool = False\n",
    "    img_size: int = 224 # 224x224 image\n",
    "    patch_size: int = 16 # 16x16 patches\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config.img_size\n",
    "        self.patch_size = config.patch_size\n",
    "        assert self.img_size % self.patch_size == 0\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=config.n_embd, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) # [B, C=3, S=224, S=224] -> [B, C=768, N=14, N=14] \n",
    "        x = x.flatten(2) # [B, C=768, N=14, N=14] -> [B, C=768, N**2=T=196] \n",
    "        x = x.transpose(1,2)  #[B, C, T] -> [B, T=196, C=768] \n",
    "\n",
    "        return x\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"vision transformer encoder for image embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert config.img_size % config.patch_size == 0\n",
    "        self.n_patch = (config.img_size // config.patch_size)**2 # N**2 = (224/16)**2 = 196\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.n_embd)) # extra learnable token\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            patch_emb = PatchEmbedding(config),\n",
    "            pos_emb = nn.Embedding(self.n_patch + 1, config.n_embd), # +1 for cls_token\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.proj = nn.Linear(config.n_embd, config.out_dim, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        # Use pytorch default LayerNorm & Conv2d init\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer.patch_emb(x) # [B, C=3, S=224, S=224] -> [B, T=196, C=768]\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # add cls_token to each batch\n",
    "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1) # [B, 197, C=768]\n",
    "        # add positional embedding\n",
    "        pos_emb = self.transformer.pos_emb(torch.arange(T+1, device=x.device)) # [197, C=768]\n",
    "        x = x + pos_emb # [B, 197, C=768]\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x) # [B, 197, C=768]\n",
    "        x = self.transformer.ln_f(x) # [B, 197, C=768]\n",
    "        cls_token = x[:, 0, :] # [B, 197, C=768] -> [B, C=768]\n",
    "        out = self.proj(cls_token) # [B, C=768] -> [B, C=512]\n",
    "        out = out / out.norm(dim=-1, keepdim=True) # normalize to unit length for cosine similarity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9f1a9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image utils\n",
    "\n",
    "vision_config = VisionConfig()\n",
    "\n",
    "# image to tensor\n",
    "itot = transforms.Compose([\n",
    "  transforms.Resize((vision_config.img_size, vision_config.img_size)),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "# tensor to image\n",
    "ttoi = transforms.ToPILImage()\n",
    "\n",
    "# sample image loader\n",
    "def load_image(name):\n",
    "    dir = \"./data/sample\"\n",
    "    file = f\"{dir}/{name}\"\n",
    "    return itot(Image.open(file).convert(\"RGB\"))\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    image = ttoi(image)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e21ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels minibatch shape (B, L=77): torch.Size([3, 77])\n",
      "label_emb shape (B, C=512): torch.Size([3, 512])\n",
      "\n",
      "images minibatch shape (B, C=3, H=224, W=224): torch.Size([3, 3, 224, 224])\n",
      "image_emb shape (B, C=512): torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Verify TextDecoder and VisionEncoder output shapes\n",
    "# ------------------------------\n",
    "\n",
    "# TextDecoder\n",
    "text_config = TextConfig()\n",
    "enc = TextTokenizer(text_config)\n",
    "model = TextDecoder(text_config)\n",
    "\n",
    "labels = torch.stack([\n",
    "    enc.encode(\"a boy and a girl\"),\n",
    "    enc.encode(\"a red ball\"),\n",
    "    enc.encode(\"a boy and a girl playing soccer in the park with a red ball\"),\n",
    "])\n",
    "\n",
    "print(f\"labels minibatch shape (B, L=77): {labels.shape}\")\n",
    "\n",
    "label_emb = model(labels)\n",
    "print(f\"label_emb shape (B, C=512): {label_emb.shape}\")\n",
    "print()\n",
    "\n",
    "# VisionEncoder\n",
    "images = torch.stack([\n",
    "    load_image(\"1.jpg\"),\n",
    "    load_image(\"2.jpg\"),\n",
    "    load_image(\"1.jpg\")\n",
    "])\n",
    "\n",
    "print(f\"images minibatch shape (B, C=3, H=224, W=224): {images.shape}\")\n",
    "\n",
    "vision_encoder = VisionEncoder(vision_config)\n",
    "image_emb = vision_encoder(images)\n",
    "\n",
    "print(f\"image_emb shape (B, C=512): {image_emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb26dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[ 0.2673,  0.5345,  0.8018],\n",
      "        [-0.3369,  0.4211,  0.8422],\n",
      "        [ 0.5774, -0.5774,  0.5774]])\n",
      "y:\n",
      "tensor([[ 0.2673,  0.5345,  0.8018],\n",
      "        [-0.2673,  0.5345,  0.8018],\n",
      "        [ 0.2673, -0.5345,  0.8018]])\n",
      "x @ y.T:\n",
      "tensor([[1.0000, 0.8571, 0.4286],\n",
      "        [0.8103, 0.9903, 0.3601],\n",
      "        [0.3086, 0.0000, 0.9258]])\n",
      "logits:\n",
      "tensor([[14.2857, 12.2449,  6.1224],\n",
      "        [11.5753, 14.1475,  5.1446],\n",
      "        [ 4.4087,  0.0000, 13.2260]])\n",
      "loss:\n",
      "l_x = 0.0654 | l_y = 0.0682 | loss = 0.0668\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Toy example: cosine similarity & contrastive loss\n",
    "# ------------------------------\n",
    "\n",
    "# example (B=3, C=3) matrix 1\n",
    "x = torch.tensor([[1, 2, 3], [-4, 5, 10], [1, -1, 1]], dtype=torch.float32)\n",
    "x = x / x.norm(dim=-1, keepdim=True)\n",
    "# example (B=3, C=3) matrix 2\n",
    "y = torch.tensor([[2, 4, 6], [-2, 4, 6], [2, -4, 6]], dtype=torch.float32)\n",
    "y = y / y.norm(dim=-1, keepdim=True)\n",
    "print(f\"x:\\n{x}\")\n",
    "print(f\"y:\\n{y}\")\n",
    "\n",
    "# cosine similarity matrix\n",
    "dot_product = x @ y.T \n",
    "print(f\"x @ y.T:\\n{dot_product}\")\n",
    "\n",
    "# scale by learnable temperature \n",
    "logit_scale = torch.ones([]) * np.log(1 / 0.07)\n",
    "logits = dot_product * logit_scale.exp()\n",
    "print(f\"logits:\\n{logits}\")\n",
    "\n",
    "# contrastive loss\n",
    "targets = torch.arange(x.shape[0])\n",
    "l_x = F.cross_entropy(logits, targets)\n",
    "l_y = F.cross_entropy(logits.T, targets)\n",
    "loss = (l_x + l_y) / 2.0\n",
    "print(f\"loss:\\nl_x = {l_x:.4f} | l_y = {l_y:.4f} | loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CLIP Model\n",
    "# ------------------------------\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self, text_config, vision_config):\n",
    "        super().__init__()\n",
    "        self.text_decoder = TextDecoder(text_config)\n",
    "        self.vision_encoder = VisionEncoder(vision_config)\n",
    "        # learnable temperature parameter (initialized to match CLIP)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, labels, images):\n",
    "        # embed labels & images\n",
    "        label_emb = self.text_decoder(labels)\n",
    "        image_emb = self.vision_encoder(images)\n",
    "        # normalize embeddings\n",
    "        label_emb = label_emb / label_emb.norm(dim=-1, keepdim=True)\n",
    "        image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
    "        # clamp temperature\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=np.log(100))\n",
    "        logits = logit_scale.exp() * label_emb @ image_emb.T\n",
    "        # compute contrastive loss\n",
    "        targets = torch.arange(logits.shape[0])\n",
    "        loss = (F.cross_entropy(logits, targets) + F.cross_entropy(logits.T, targets)) / 2.0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0cb6ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1142\n"
     ]
    }
   ],
   "source": [
    "model = CLIP(text_config, vision_config)\n",
    "loss = model(labels, images)\n",
    "print(f\"loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
