{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c26614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ffa0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text Decoder (GPT-style) \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class TextConfig:\n",
    "  block_size: int = 77\n",
    "  vocab_size: int = 50258 # from TextTokenizer\n",
    "  n_layer: int = 6\n",
    "  n_head: int = 8\n",
    "  n_embd: int = 512\n",
    "  out_dim: int = 512\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" multiple self-attention heads in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, batched together\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, sequence length, n_embd\n",
    "        # calculate query, key, value for all heads in a batch\n",
    "        # C = n_head * head_size, eg n_head = 12, head_size = 64, so C = 768\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        q = q.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        v = v.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        \n",
    "        # use flash attention instead of manually implemented attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # (B, n_head, T, head_size)\n",
    "        \n",
    "        y = y.transpose(1, 2).reshape(B, T, -1) # (B, n_head, T, head_size) -> (B, T, n_head * head_size)\n",
    "\n",
    "        y = self.c_proj(y) \n",
    "        return y    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Linear layer + non-linearity to add compute after multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # expand onto higher dimensional space\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd) # project back down to model's embedding dimensionality \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: Communication followed by computation, with residual connection (x +) \"\"\" \n",
    "\n",
    "    def __init__(self, config, is_decoder=True):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd) # ToDo: Understand\n",
    "        self.attn = Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd) # ToDo: Understand\n",
    "        self.mlp = MLP(config) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "\n",
    "  def __init__(self, config: TextDecoderConfig):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.transformer = nn.ModuleDict(dict(\n",
    "      wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "      wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "      h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "      ln_f = nn.LayerNorm(config.n_embd),\n",
    "    ))\n",
    "    self.proj = nn.Linear(config.n_embd, config.out_dim, bias=False)\n",
    "\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      std = 0.02\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    # Default LayerNorm init in pytorch matches GPT2, so no need to change\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    B, T = idx.shape\n",
    "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "      \n",
    "    # idx and targets are both (B, T) tensor of integers\n",
    "    token_emb = self.transformer.wte(idx) # (B, T, C = n_embd)\n",
    "    pos_emb = self.transformer.wpe(torch.arange(T, device=idx.device)) # (T, C = n_embd)\n",
    "    x = token_emb + pos_emb # (B, T, C = n_embd)\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x) # (B, T, C = n_embd)\n",
    "    x = self.transformer.ln_f(x)\n",
    "    # Find first occurrence of eot_token_id (50256)\n",
    "    # TODO: don't hardcode EOT token id\n",
    "    eot_positions = (idx == 50256).int().argmax(dim=-1)  # (B,)\n",
    "    x = x[torch.arange(x.shape[0]), eot_positions]  # (B, C = n_embd)\n",
    "    x = self.proj(x) # (B, C = out_dim)\n",
    "    x = x / x.norm(dim=-1, keepdim=True) # normalize to unit length for cosine similarity\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a595ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer: \n",
    "    \"\"\" \n",
    "    tiktoken Wrapper\n",
    "    Not quite CLIP tokenizer, but approximates it using GPT-2 tokenizer \n",
    "    Vocab size = GPT-2 vocab size (50257) + 1 (for new SOT token) = 50258\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "        # Special tokens\n",
    "        self.eot_token = \"<|endoftext|>\"\n",
    "        self.pad_token = self.eot_token\n",
    "        self.sot_token = \"<|startoftext|>\"\n",
    "        self.eot_token_id = 50256 # already exists in GPT-2 tokenizer\n",
    "        self.pad_token_id = self.eot_token_id\n",
    "        self.sot_token_id = self.eot_token_id + 1 # doesn't exist in GPT-2 tokenizer\n",
    "        \n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [self.sot_token_id]\n",
    "        text_enc = self.enc.encode(text)\n",
    "        if len(text_enc) + 2 > self.block_size:\n",
    "            tokens.extend(text_enc[:self.block_size - 2])\n",
    "        else:\n",
    "            tokens.extend(text_enc)\n",
    "            if len(tokens) < self.block_size:\n",
    "                tokens.extend([self.pad_token_id] * (self.block_size - 1 - len(tokens)))\n",
    "        tokens.extend([self.eot_token_id])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids, include_special_tokens=True):\n",
    "        result = \"\"\n",
    "        for id in ids:\n",
    "            if id == self.sot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.sot_token\n",
    "            elif id == self.eot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.eot_token\n",
    "            else:\n",
    "                result += self.enc.decode([id])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c773143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([3, 77])\n",
      "----\n",
      "Encoding preview: tensor([50257,    64,  2933,   290,   257,  2576, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "Encoding preview: tensor([50257,    64,  2266,  2613, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "Encoding preview: tensor([50257,    64,  2933,   290,   257,  2576,  2712, 11783,   287,   262,\n",
      "         3952,   351,   257,  2266,  2613, 50256, 50256, 50256, 50256, 50256])\n",
      "----\n",
      "Text embedding shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    \"a boy and a girl\",\n",
    "    \"a red ball\",\n",
    "    \"a boy and a girl playing soccer in the park with a red ball\",\n",
    "]\n",
    "\n",
    "enc = TextTokenizer(TextConfig())\n",
    "encodings = [torch.tensor(enc.encode(label), dtype=torch.long) for label in labels]\n",
    "\n",
    "batch = torch.stack(encodings)\n",
    "print(f\"Batch shape: {batch.shape}\\n----\")\n",
    "\n",
    "for encoding in encodings:\n",
    "    print(f\"Encoding preview: {encoding[:20]}\")\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "model = TextDecoder(TextConfig())\n",
    "text_emb = model(batch)\n",
    "print(f\"Text embedding shape: {text_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d84fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
