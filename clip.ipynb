{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c26614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df8178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Shared Transformer Blocks \"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Linear layer + non-linearity to add compute after multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # expand onto higher dimensional space\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd) # project back down to model's embedding dimensionality \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" multiple self-attention heads in parallel \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, batched together\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, sequence length, n_embd\n",
    "        # calculate query, key, value for all heads in a batch\n",
    "        # C = n_head * head_size, eg n_head = 12, head_size = 64, so C = 768\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        q = q.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        v = v.view(B, T, self.n_head, -1).transpose(1, 2) #(B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        \n",
    "        # use flash attention instead of manually implemented attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=self.is_decoder) # (B, n_head, T, head_size)\n",
    "        \n",
    "        y = y.transpose(1, 2).reshape(B, T, -1) # (B, n_head, T, head_size) -> (B, T, n_head * head_size)\n",
    "\n",
    "        y = self.c_proj(y) \n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: Communication followed by computation, with residual connection (x +) \"\"\" \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text Decoder (GPT-style) \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class TextConfig:\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 512\n",
    "    out_dim: int = 512\n",
    "    is_decoder: bool = True\n",
    "    block_size: int = 77\n",
    "    vocab_size: int = 50258 # from TextTokenizer\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.proj = nn.Linear(config.n_embd, config.out_dim, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        # Use pytorch default LayerNorm init\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        token_emb = self.transformer.wte(idx) # (B, T, C = n_embd)\n",
    "        pos_emb = self.transformer.wpe(torch.arange(T, device=idx.device)) # (T, C = n_embd)\n",
    "        x = token_emb + pos_emb # (B, T, C = n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x) # (B, T, C = n_embd)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        # Find first occurrence of eot_token_id (50256)\n",
    "        # TODO: don't hardcode EOT token id\n",
    "        eot_positions = (idx == 50256).int().argmax(dim=-1)  # (B,)\n",
    "        eot_token = x[torch.arange(x.shape[0]), eot_positions]  # (B, C = n_embd)\n",
    "        out = self.proj(eot_token) # (B, C = out_dim)\n",
    "        out = out / out.norm(dim=-1, keepdim=True) # normalize to unit length for cosine similarity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87a595ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Text Tokenizer \"\"\"\n",
    "\n",
    "class TextTokenizer: \n",
    "    \"\"\" \n",
    "    tiktoken Wrapper\n",
    "    Not quite CLIP tokenizer, but approximates it using GPT-2 tokenizer \n",
    "    Vocab size = GPT-2 vocab size (50257) + 1 (for new SOT token) = 50258\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "        # Special tokens\n",
    "        self.eot_token = \"<|endoftext|>\"\n",
    "        self.pad_token = self.eot_token\n",
    "        self.sot_token = \"<|startoftext|>\"\n",
    "        self.eot_token_id = 50256 # already exists in GPT-2 tokenizer\n",
    "        self.pad_token_id = self.eot_token_id\n",
    "        self.sot_token_id = self.eot_token_id + 1 # doesn't exist in GPT-2 tokenizer\n",
    "        \n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [self.sot_token_id]\n",
    "        text_enc = self.enc.encode(text)\n",
    "        if len(text_enc) + 2 > self.block_size:\n",
    "            tokens.extend(text_enc[:self.block_size - 2])\n",
    "        else:\n",
    "            tokens.extend(text_enc)\n",
    "            if len(tokens) < self.block_size:\n",
    "                tokens.extend([self.pad_token_id] * (self.block_size - 1 - len(tokens)))\n",
    "        tokens.extend([self.eot_token_id])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids, include_special_tokens=True):\n",
    "        result = \"\"\n",
    "        for id in ids:\n",
    "            if id == self.sot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.sot_token\n",
    "            elif id == self.eot_token_id:\n",
    "                if include_special_tokens:\n",
    "                    result += self.eot_token\n",
    "            else:\n",
    "                result += self.enc.decode([id])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8c773143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([3, 77])\n",
      "text_emb shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Playing with TextDecoder and TextTokenizer\n",
    "\"\"\"\n",
    "\n",
    "labels = [\n",
    "    \"a boy and a girl\",\n",
    "    \"a red ball\",\n",
    "    \"a boy and a girl playing soccer in the park with a red ball\",\n",
    "]\n",
    "\n",
    "enc = TextTokenizer(TextConfig())\n",
    "model = TextDecoder(TextConfig())\n",
    "\n",
    "encodings = [torch.tensor(enc.encode(label), dtype=torch.long) for label in labels]\n",
    "batch = torch.stack(encodings)\n",
    "print(f\"batch shape: {batch.shape}\")\n",
    "\n",
    "text_emb = model(batch)\n",
    "print(f\"text_emb shape: {text_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "231d84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vision Encoder \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class VisionConfig:\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 768\n",
    "    out_dim: int = 512\n",
    "    is_decoder: bool = False\n",
    "    img_size: int = 224 # 224x224 image\n",
    "    patch_size: int = 16 # 16x16 patches\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config.img_size\n",
    "        self.patch_size = config.patch_size\n",
    "        assert self.img_size % self.patch_size == 0\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=config.n_embd, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) # [B, C=3, S=224, S=224] -> [B, C=768, N=14, N=14] \n",
    "        x = x.flatten(2) # [B, C=768, N=14, N=14] -> [B, C=768, N**2=T=196] \n",
    "        x = x.transpose(1,2)  #[B, C, T] -> [B, T=196, C=768] \n",
    "\n",
    "        return x\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    # TODO: add positional embedding & class token\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert config.img_size % config.patch_size == 0\n",
    "        self.n_patch = (config.img_size // config.patch_size)**2 # N**2 = (224/16)**2 = 196\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.n_embd)) # extra learnable token\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            patch_emb = PatchEmbedding(config),\n",
    "            pos_emb = nn.Embedding(self.n_patch + 1, config.n_embd), # +1 for cls_token\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.proj = nn.Linear(config.n_embd, config.out_dim, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        # Use pytorch default LayerNorm & Conv2d init\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer.patch_emb(x) # [B, C=3, S=224, S=224] -> [B, T=196, C=768]\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # add cls_token to each batch\n",
    "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1) # [B, 197, C=768]\n",
    "        # add positional embedding\n",
    "        pos_emb = self.transformer.pos_emb(torch.arange(T+1, device=x.device)) # [197, C=768]\n",
    "        x = x + pos_emb # [B, 197, C=768]\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x) # [B, 197, C=768]\n",
    "        x = self.transformer.ln_f(x) # [B, 197, C=768]\n",
    "        cls_token = x[:, 0, :] # [B, 197, C=768] -> [B, C=768]\n",
    "        out = self.proj(cls_token) # [B, C=768] -> [B, C=512]\n",
    "        out = out / out.norm(dim=-1, keepdim=True) # normalize to unit length for cosine similarity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "83e21ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_emb shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Playing with VisionEncoder\n",
    "\"\"\"\n",
    "\n",
    "imgs = torch.randn(3, 3, 224, 224)\n",
    "\n",
    "\n",
    "vision_encoder = VisionEncoder(VisionConfig())\n",
    "vision_emb = vision_encoder(imgs)\n",
    "print(f\"vision_emb shape: {vision_emb.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
